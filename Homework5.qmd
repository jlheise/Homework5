---
title: "Homework5"
author: "Jessie Heise"
format: pdf
editor: visual
---

## Task 1: Conceptual Questions

### 1. What is the purpose of using cross-validation when fitting a random forest model?

To avoid overfitting.

### 2. Describe the bagged tree algorithm.

The bagged tree algorithm fits many trees on bootstrap samples and combines the predictions.

### 3. What is meant by a general linear model?

A statistical model used on non-normal data.

### 4. When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

Allows for the effect of one variable to depend on the value of another.

### 5. Why do we split our data into a training and test set?

We want to be able to use models to predict well for observations it has yet to see and avoid overfitting the model based on the test data which is why the training set is used to traing the model and the test set is used to judge effectiveness of the model.

## Task 2: Data Prep

### Packages & Library

```{r, echo = FALSE, message = FALSE, warning = FALSE, include = FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)

# Read in csv data
heart_data <- read.csv('heart.csv')
```

### Run a report summary()

```{r}
summary(heart_data)
```

### Report summary questions

#### 1. What type of variable (in R) is Heart Disease? Categorical or Quantitative?

Numeric. Categorical.

#### 2. Does this make sense? Why or why not.

This does not make sense because the heart disease variable tells whether or not someone has heart disease- essentially a yes or a no. This data should not be numeric.

### Continued data prep

```{r}
# Change HeartDisease to logical, drop ST_Slope, HeartDisease
new_heart <- heart_data |>
  mutate(HeartDisease_pres = as.factor(HeartDisease)) |>
  select(-HeartDisease, -ST_Slope)
```

## Task 3: EDA

### Model someone's age as a function of heart disease and their max heart rate

```{r}
# Create appropriate scatterplot to visualize this relationship
ggplot2::ggplot(data = new_heart, aes(x= MaxHR, y = Age, color = HeartDisease_pres)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Age as a function of heart disease and maximum heart rate",
    x = "Maximum Heart Rate",
    y = "Age",
    color = "Presence of Heart Disease"
  ) +
  scale_color_viridis_d()
```

### Based on visual evidence, do you think an interaction model or an additive model is more appropriate?

Based on the visual evidence, an interactive model is more appropriate because the lines for presence of heart disease cross, suggesting an interactive effect.

## Task 4: Testing & Training

```{r}
# Split data into a training and test set. Seed = 101. 80-20 split.
set.seed(101)
heart_split <- initial_split(new_heart, prop = 0.8)
train <- training(heart_split)
test <- testing(heart_split)
```

## Task 5: OLS and LASSO

### Fit an interaction model

```{r}
# Fit an interaction model with age as response, max hr + heart disease as explanatory variables using the training data set using OLS regression. report summary output
ols_mlr <- lm(Age ~ HeartDisease_pres * MaxHR, data = train)
summary(ols_mlr)

# Test model on testing data set
ols_predict <- predict(ols_mlr, newdata = test)

# Calculate RMSE for OLS
rmse_vec(test$Age, predict(ols_mlr, newdata = test))
```

### See if LASSO has better predictive performance

```{r}
# Use CV to select best tuning parameter. Evaluate LASSO model on testing data set
heart_CV_folds <- vfold_cv(new_heart, 10)

my_metrics <- metric_set(rmse)

LASSO_recipe <- recipe(Age ~ HeartDisease_pres + MaxHR, 
                      data = new_heart) |>
  step_dummy(HeartDisease_pres) |>
  step_normalize(MaxHR) |>
  step_interact(~ starts_with("HeartDisease_pres"):starts_with("MaxHR_"))
LASSO_recipe

LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

LASSO_wkf <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(LASSO_spec)
LASSO_wkf

#A warning will occur for one value of the tuning parameter, safe to ignore
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = heart_CV_folds,
            grid = grid_regular(penalty(), levels = 5),
            metrics = my_metrics) 

LASSO_grid

LASSO_grid[1, ".metrics"][[1]]

LASSO_grid |>
  collect_metrics() |>
  filter(.metric == "rmse")

lowest_rmse <- LASSO_grid |>
  select_best(metric = "rmse")

lrmse <- as.numeric(lowest_rmse$penalty)


LASSO_final <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(new_heart)
tidy(LASSO_final)

```

I would expect the RMSE calculations to be different since the intercepts of the models are different.

### Compare RMSEs:

```{r}
ols_mlr |>
  predict(test) |>
  rmse_vec(truth = test$Age)

LASSO_final |>
  predict(test) |>
  pull() |>
  rmse_vec(truth = test$Age)
```

I think the RMSE calculations are roughly the same even though the coefficients for each model are
different because both models might average the same errors.

## Task 6: Logistic Regression

```{r}
set.seed(3557)
heart_data <- heart_data |>
  mutate(HeartDisease = factor(HeartDisease)) 
heart_split <- initial_split(heart_data, prop = 0.8) 
heart_train <- training(heart_split) 
heart_test <- testing(heart_split) 
heart_CV_folds <- vfold_cv(heart_train, 10)

LR1_rec <- recipe(HeartDisease ~ RestingBP + RestingECG, data = heart_train) |>  
  step_normalize(all_numeric(), -HeartDisease) |>  
  step_dummy(RestingECG) 
LR2_rec <- recipe(HeartDisease ~ ChestPainType + MaxHR + ExerciseAngina, data = heart_train) |>  
  step_normalize(all_numeric(), -HeartDisease) |>  
  step_dummy(ChestPainType, ExerciseAngina) 
LR2_rec |> 
  prep(heart_train) |> 
  bake(heart_train) |> 
  colnames()
LR_spec <- logistic_reg() |>  
  set_engine("glm")
LR1_wkf <- workflow() |>  
  add_recipe(LR1_rec) |>  
  add_model(LR_spec) 
LR2_wkf <- workflow() |>  
  add_recipe(LR2_rec) |>  
  add_model(LR_spec)
LR1_fit <- LR1_wkf |>  
  fit_resamples(heart_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
LR2_fit <- LR2_wkf |>  
  fit_resamples(heart_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics()) |>  
  mutate(Model = c("Model1", "Model1", "Model2", "Model2")) |>
  select(Model, everything())
mean(heart_train$HeartDisease == "1")
```

The best performing model is Model 2 because it has higher accuracy and log loss closer to 0. Model 2 uses Chest Pain Type, Max HR, and Exercise in Angina.

```{r}
final_model <- LR2_wkf |>  
  fit(heart_data) 
tidy(final_model)
LR_train_fit <- LR2_wkf |>  
  fit(heart_train) 
LR2_wkf |>  
  last_fit(heart_split, metrics = metric_set(accuracy, mn_log_loss)) |>
  collect_metrics()
```

Check how well Model 2 does on the test set using the confusionMatrix() function:

```{r}
LR2_test_fit <- LR2_wkf |>
  fit(heart_test)

conf_mat(heart_test |> mutate(estimate = LR_test_fit |> predict(heart_test) |> pull()), HeartDisease, estimate) 

TP <- 81
FN <- 16
TN <- 58
FP <- 29

sensitivity <- TP/(TP+FN)
specificity <- TN/(TN+FP)

print(sensitivity)
print(specificity)
```

Since sensitivity is the true positive rate, the model accurately predicts the presence of heart disease in 83.5% of the time. Since Specificity is the true negative rate, the model accurately predicts the absence of heart disease 66.7% of hte time. The model is better at predicting heart disease than not predicting it.
